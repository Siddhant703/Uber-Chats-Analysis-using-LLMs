# -*- coding: utf-8 -*-
"""Final_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuaSkC0pibhsuRZHRGKIbmBy23XYOYv9
"""

!pip install openai
!pip install openpyxl

import pandas as pd
import numpy as np

"""# Read File

# Read File

- Method 1 is to read a single excel/csv file
- Method 2 is to read different files all at once and merge them to create a consolidated file


### Note: ‘content’ should be updated with Sid Results -> Data Files
1. Confessions come from Stories
"""

### METHOD 1

df = pd.read_excel("/content/India Leadership Class.xlsx")
df
# num = 0 #set this value to the number of row you want to analyzie
# df = df[:num]

### METHOD 2

names = ['/content/final_collected_data_Confessions (1).csv',
         '/content/final_collected_data_Advice.csv',
         '/content/final_collected_data_Complaints.csv']

num = 50000 #set this value to the number of rows you want analyze in each dataset
data_frames = [pd.read_csv(i).iloc[:num] for i in names]
dd = pd.concat(data_frames, ignore_index=True)

"""# Classification (into 1s or 0s)"""

prompt = '''

I want you to classify the tone of the text into a relevant tone based on the text. Please do a tone analysis on this text. the context of the task is that I want to do a tonal analysis.
Please only classify the text as one tone only and only output one tone for the text. Please give your output in format - [tone].  Please enclose your output within square brackets only and dont give any unnecessary information or explanation as to why you chose that label simply give me the labels in square brackets. If the message doesnt fit into any tone then please output none in the brackets for tones but please try to fit the message into a tones but dont force it
Please make sure the tone you output is a simple standard tone and generic tone. Dont output message specific topics.

Uber Message

'''

import os
from openai import OpenAI
client = OpenAI(api_key = '')

def generate(x):
  completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
      {"role": "user", "content": prompt + x}
    ]
  )

  return completion.choices[0].message.content

"""You can set different values for max_workers. Suggested value is 50 but you can set to higher if you have the highest version of colab which gives higher gpu limits"""

from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import as_completed


# Function to process the data in parallel and keep results ordered
def parallel_generate(content_list):
    results = [None] * len(content_list)
    x = 0  # Pre-allocate a list for ordered results
    with ThreadPoolExecutor(max_workers=500) as executor:  # Adjust max_workers as needed
        future_to_index = {executor.submit(generate, content): idx for idx, content in enumerate(content_list)}
        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                results[idx] = future.result()  # Place result in the correct index
                print(results[idx])
                print(idx)

                x+1
            except Exception as e:
                print(f"Error occurred at index {idx}: {e}")
    return results

"""Input the column name on which you want to run classification

### India file has 'text', where as current US files have 'content'.
"""

results = parallel_generate(df['text'].to_list())

"""Make sure the label has no comma"""

s1 = []
s2 = []
for i in results:
  s1.append(str(i).replace("]","").replace("[","").split(",")[0])
  # s2.append(''.join(str(i).replace("]","").replace("[","").split(",")[1:]))

# result_df = pd.DataFrame(messages, columns=['combined_text'])
# df['Results'] = results
df['Tone'] = s1
# df['Explanation'] = s2

###Check Point 1
df.to_excel("Tone.xlsx")

"""# Calculating accuracy, recall and precision of classification results.

**Calculating accuracy/recall/precision**

## replace no with not if your model predictions contain that for the negative labels (catch negative labels)
"""

d = []
for i in df['Label']:
  if "no" in str(i).lower(): #replace no with not if your model predictions contain that for the negative labels
    d.append(0)
  else:
    d.append(1)
df['value'] = d

"""### Input column name which has the manual predictions (column name of the golden predicitions (ground truth) of the test sets)."""

### Input column name which has the manual predictions
col = ""

###Accuracy
n = []
for i in range(len(df)):
  try:
    if int(df[col][i]) == int(df['value'][i]):
      n.append('1')
  except:
    print(df[col][i], df['value'][i])
print(len(n)*100/len(df))

###Recall
dt = df[df[col] == 1]  # Filter where col == 1
n = (dt[col].astype(str) == dt['value'].astype(str)).sum()
recall = (n / len(dt)) * 100
print(recall)

tp = ((dd[col] == 1) & (dd['value'] == 1)).sum()

# Calculate False Positives (FP): predicted = 1, actual = 0
fp = ((dd[col] == 1) & (dd['value'] == 0)).sum()

# Precision formula
precision = (tp / (tp + fp)) * 100 if (tp + fp) > 0 else 0

print(f'Precision: {precision:.2f}%')

"""# Run Analysis (Clustering/Topic/Tone) via Chunking

Run this cell only if you want to run clustering or any analysis where you only want to include the 1s (don't run if not running clustering)
"""

####### RUN THIS ONLY IF YOU WANT TO RUN FOR 1s (in the case of clustering for example) ##########
df = df[df['value'] == 1]
text = df['Text Column'] ##Input column name of orignal messages/text (not the results)

"""First Step - To run the first iteration"""

text = []
x = "Explanation"
for i in range(len(df)):
  t = f"\nUber Message - {df['Text Column'][i]}\nExplanation - {df[x][i]}" #Input column name of orignal messages/text (not the results)
  text.append(t)

prompt = '''

INPUT PROMPT

'''

"""Two parameters in the following code block that you can experiment with:
- Step: this controls the chunk size i.e. how many messages+explanations you want to input in each call to the model. Suggested value is 100 so that we can get a good number of clusters but if you want more specific/nuanced cluster then try experimenting between 50-100.

- max_workers: Suggested value is 50 but you can set to higher if you have the highest version of colab which gives higher gpu limits

"""

# Function to process the data in parallel and keep results ordered

def parallel_generate_ranges(prompt, text, step=50):
    num_segments = len(range(0, len(text), step))  # Calculate the number of segments
    results = [None] * num_segments  # Pre-allocate a list for ordered results

    with ThreadPoolExecutor(max_workers=100) as executor:  # Adjust max_workers as needed
        # Schedule each range segment as a separate task
        future_to_index = {
            executor.submit(generate, prompt, ''.join(str(text[i:i+step]))): idx
            for idx, i in enumerate(range(0, len(text), step))
        }
        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                results[idx] = future.result()  # Place result in the correct index
                print(results[idx])
                print(idx)
            except Exception as e:
                print(f"Error occurred at index {idx}: {e}")

    return results

r = parallel_generate_ranges(prompt, text)

"""In the following code we are running the rest of the iterations (anywhere between 2-7 depnding on initial data size) to get the final table"""

## Use step 4 from the prompt file
prompt = '''


INPUT PROMPT

'''

## This cell is expected to run mutiple times.

status = True
while status == True:
  #20 indicites the step parameter and is the suggested value for number of tables to input per api call (20 here now represents # of tables).
  r = parallel_generate_ranges(prompt, r, 20)
  if len(r) <=20:
    final_table = parallel_generate_ranges(prompt, r, 20)
    status == False

final_table