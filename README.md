# ðŸ’¬ LLM-Based Text Classification and Analysis

This project leverages large language models (LLMs) to classify, evaluate, and analyze Uber messages. It supports categorization, clustering, topic modeling, and tone detection using custom prompts and efficient parallel processing.

The goal of this project was to gain insight into gig economy behaviour by classifying Uber messages (individually) into whether they exhibit gig-specific leadership/helping/power mechanisms using detailed prompting. Furthermore, I use iterative chunking along with specific prompts to analyse tones/topics/clusters displayed in such messages.

---
## Installation

bash
pip install -r requirements.txt

---
## Project Workflow


### 1. Data Preparation

There are two ways to load the data:

#### 1.1 Method 1 â€“ Single File Processing
- A single CSV file is read.
- Control the number of rows using the `num` parameter.

#### 1.2 Method 2 â€“ Multiple File Merging
- Multiple CSV files (e.g., confessions, advice) from the Uber platform are loaded and concatenated.
- Use the `num` parameter to limit the number of rows per file.

---

### 2. Classification Process

This section classifies messages into predefined categories and generates justifications for each classification.

#### Categories:
- Power
- Leadership
- Helping

#### How it Works:
- **Prompting**: A detailed predefined prompt guides the model in how to classify each message by giving it context of how leadership/power/helping is defined in the gig economy
- **Parallel Processing**: `ThreadPoolExecutor` is used for efficient processing of multiple messages.
- **Outputs**:
  - `Label`: Category assigned to the message
  - `Explanation`: Justification for that classification

> Set the correct column name when calling `parallel_generate`, and choose the appropriate prompt from **All Prompts for Classification** based on the topic (e.g., leadership/helping/power).

---

### 3. Evaluating Model Performance

Use this section if you have a manually labeled dataset with each message manually classified into one of the 3 categories - used to evaluate prompt performance and construct final prompt

#### Metrics Calculated:
- **Accuracy**: Percentage of correct predictions
- **Recall**: Modelâ€™s ability to find all relevant cases
- **Precision**: Accuracy of the positive predictions

#### Steps:
1. Specify the column with manual labels.
2. Run the metric calculation blocks.
3. Review the printed scores.

---

### 4. Analysis: Clustering, Topic, and Tone

This section uses the GPT4o-mini model in an interative chunking manner to generate clustering, topic, and tone insights.

#### 4.1 When to Use:

- **Clustering**: Use for messages labeled `1` (power/leadership/helping), includes both messages and explanations.
- **Topic/Tone**: Use for all messages (`0s` and `1s`), includes only messages.

#### 4.2 Chunking Parameters:
- `Step Size`: Number of messages per chunk (recommended: 50â€“100 as anything more results in hallcunations by the LLM)
- `Max Workers`: Number of parallel tasks for faster processing

#### 4.3 Steps to Run Analysis:

**Step 1: Initial Table Generation (Prompt 1)**
- Uses selected messages (with explanations) to generate initial set of tables.
- Choose the appropriate prompt from **Section 2 â€“All Prompts for Analysis**.

**Step 2: Iterative Table Refinement (Prompt 2)**
- Refines the table output generated in the previous step iteratively using a different prompt optimized for summarization.
- Choose the appropriate prompt from **Section 2 â€“All Prompts for Analysis**.

---

### 5. Final Output Formatting

Used regex and in-built python functions to parse table output generated by GPT4o-mini and transforms it into a downloadable csv with a properly formatted table

---

